# Chapter 5 — Vision-Language-Action Systems

## 5.1 VLA Overview
- Combining vision, language, and action
- AI-driven decision making in robots

## 5.2 Vision Models
- Object detection (YOLO, DETR)
- Segmentation (SAM)
- Depth estimation

## 5.3 Language Models
- LLMs integration
- Instruction parsing
- Planning from text commands

## 5.4 Action Planning
- Motion generation
- Task sequencing
- Goal-directed actions

## 5.5 Perception-Action Pipeline
- Image → Text → Action
- Sensor fusion
- Real-time updates

## 5.6 Practical Use Cases
- Pick and place
- Navigation
- Human-robot interaction
